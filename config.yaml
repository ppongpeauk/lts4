# Concert2Studio Configuration
# All hyperparameters, paths, and toggle flags

# Data Configuration
data:
  data_dir: "./data"
  sample_rate: 48000  # Use 48kHz for better audio quality and noise reduction
  bit_depth: 24
  train_split: 0.8
  val_split: 0.2
  random_seed: 42
  use_synthetic: true  # Enable synthetic data for better generalization

# Audio Processing (optimized for 48kHz)
audio:
  segment_length: 2.0  # Longer segments for better context
  n_fft: 1024  # Larger FFT for better frequency resolution
  hop_length: 256
  win_length: 1024
  window: "hann"

# Model Architecture
model:
  # Spectrogram U-Net (optimized for small dataset + stereo)
  unet:
    in_channels: 513  # Frequency bins for 1024 FFT (1024//2 + 1)
    base_channels: 12  # Reduced capacity to prevent overfitting
    max_channels: 64   # Much smaller capacity for tiny dataset
    n_blocks: 2        # Fewer blocks to reduce complexity
    dilations: [1, 2]  # Simplified receptive fields
    use_attention: false  # Disable attention to reduce overfitting
    attention_heads: 2
    dropout: 0.5  # Aggressive dropout for regularization
    use_stereo: true  # Enable stereo processing

  # Enhanced UnivNet Vocoder (with noise reduction)
  use_vocoder: true  # Keep vocoder for quality enhancement
  vocoder:
    model_name: "univnet-c32"
    pretrained: false  # Train from scratch for better fit
    freeze_epochs: 25  # Freeze much longer to establish stable UNet first
    use_stereo: true  # Enable stereo output

# Training Configuration (anti-overfitting for tiny dataset)
training:
  batch_size: 2  # Small but efficient batches
  num_epochs: 50  # More training with better regularization
  learning_rate: 5e-6  # Much more conservative for tiny dataset
  weight_decay: 1e-3  # Increased L2 regularization to prevent overfitting
  beta1: 0.9  # Standard Adam parameters
  beta2: 0.999
  warmup_steps: 500  # Much longer warmup for stability
  early_stopping_patience: 5  # Less patience to stop overfitting quickly
  gradient_accumulation_steps: 4  # Effective batch size 8
  max_grad_norm: 0.3  # Even stricter gradient clipping
  scheduler: "linear"  # Linear decay instead of cosine for stability

# Loss Configuration (simplified for stability)
loss:
  l1_weight: 1.0  # Focus primarily on L1 reconstruction
  multires_stft_weight: 0.0  # Disable spectral loss initially for stability
  vggish_weight: 0.0  # Disabled heavy perceptual loss
  adversarial_weight: 0.0  # Disabled for stability

  # Multi-resolution STFT scales (reduced for stability)
  stft_scales: [512, 1024, 2048]

# Data Augmentation (aggressive anti-overfitting for tiny dataset)
augmentation:
  gain_jitter_db: 2.0  # Increased variation to improve generalization
  gain_jitter_prob: 0.7  # Higher probability for more diversity
  pink_noise_prob: 0.4  # More noise augmentation
  pink_noise_level: -45  # Slightly more noticeable noise
  time_shift_ms: 40  # More time variation
  time_shift_prob: 0.6  # Higher probability
  # Advanced augmentation techniques from research
  mixup_prob: 0.3  # Enable mixup for better generalization
  mixup_alpha: 0.3  # Moderate mixing
  spec_augment_prob: 0.4  # Enable spectrogram augmentation
  freq_mask_prob: 0.2
  time_mask_prob: 0.2

# Checkpointing and Logging
checkpoint:
  save_interval_minutes: 15
  save_interval_steps: 500
  max_checkpoints: 5
  resume_from: null  # path to checkpoint to resume from

# Hardware Configuration
hardware:
  num_workers: 16
  pin_memory: true
  persistent_workers: true
  compile_model: false  # Disabled to avoid CUDA compilation issues
  channels_last: false  # Not applicable to audio models

# Paths
paths:
  output_dir: "./outputs"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"
